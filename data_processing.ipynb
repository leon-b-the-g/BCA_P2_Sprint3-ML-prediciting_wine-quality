{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (573506680.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    url=\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#import scikit\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "#load dataset and trnasform to df\n",
    "url=\n",
    "url = + url.split(\"/\")[-2]\n",
    "df_base = pd.read_csv(url)\n",
    "\n",
    "#Base dataframe \n",
    "df_base.head()\n",
    "\n",
    "#Filtered dataframe\n",
    "init_filter_df_base=df_base.loc([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe data types\n",
    "df_base.dtypes()\n",
    "\n",
    "#Description of data types\n",
    "df_base.describe.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe duplicates check\n",
    "df-base.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in string_col:\n",
    "    print(f\"This [col]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m groupby_base_df \u001b[38;5;241m=\u001b[39m df_base\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeartDisease\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeartDisease\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mcount()]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_base' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "groupby_base_df = df_base.groupby([\"Sex\",\"HeartDisease\"])[\"HeartDisease\".count()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm[\"sex\"].replace ({\"M\":1,\"F\":0}, inplace = True)\n",
    "dfm[\"ExcerciseAngina\"].replace({\"Y\":1,\"N\":0},inplace=True)\n",
    "\n",
    "cat_cols = dfm.select_dtypes(\"Object\")\n",
    "cot_col_encode = pd.get_dummies(cat_cols)\n",
    "\n",
    "dfm.drop(columns=cat_cols, axis = 1, inplace = True)\n",
    "\n",
    "dfml = pd.concate([dfm, cat_col_encode], axis = 1) # make sure to import df in the first cell!!!\n",
    "\n",
    "dfml.head()\n",
    "\n",
    "##RESEARCH Hot encoder \n",
    "#Lets you drop a diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split data into X and y\n",
    "X = dfml.drop(\"HeartDisease\", axis = 1) ### ALL other features of the model\n",
    "    ### CAPITAL X is always the denomination for the mathematical expression of multiple bits of data (i.e. multiple features) \n",
    "y = dfml[\"HeartDisease\"] ### y is the TARGET feature correl\n",
    "    ### LOWERCASE y \n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.20, random_state = 42) # 42 from answer to the universe and everything in it (hitchikers guide to the galaxy)\n",
    "\n",
    "\n",
    "#There are a few ways to split data, but 42 is the industry standard, if you change random_state, \n",
    "# keep it consistent across several different data splits!!!\n",
    "#SPLITS RANDOMLY\n",
    "    #SPLITS SELECTED FEATURE COLUMN TO GO INTO TEST DATA SPLIT\n",
    "\n",
    "\n",
    "#Common Data split (1000 samples):\n",
    "#train 800 (80%)\n",
    "    #X-train: outputs a df for the selected important features\n",
    "    #y-train  : outputs a series of \"yes/no\" (0 or 1) booleans for the selected feature we are predicting\n",
    "\n",
    "\n",
    "#test 200 (20%):\n",
    "    #x-test : the boolean for a \"true\" example of the feature we are trying to predict from the training model FROM THE REST OF THE TABLE of the original dataset\n",
    "    #y-test ; works with y_predict, y _test is used to check the predictions,\n",
    "    #  we never showed this true value to the model! Only we know t (kinda like a double blind trial)\n",
    "\n",
    "\n",
    "\n",
    "###EVALUATIONS\n",
    "#We get our predictions by comparing y-predict with y-test!!!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#Logistic Regression\n",
    "\n",
    "LR_model = LogisticRegression(random_state = 42, max_iter = 10000) # you cant really overshoot, it just takes longer, the more rows you have, if you have a large max_iter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "LR_model.fit(x_train,y_train)\n",
    "LogisticRegression(max_iter= 10000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicitions\n",
    "preds = LR_model.predict(x_test) #Outputs an array of predictions (NOT a DF!!!)\n",
    "    #uses an x_test to ork with the rows from the df we inputed, the series is indexed according to the index of the dataframe,\n",
    "    #  so you can refer back to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, accuracy_score # rmse root mean square error\n",
    "\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT STATEMENT CONFUSION MATRIX \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "confusion_matrix(y_test,preds)\n",
    "# outputs a matrix for each row and how many times it got the prediction right\n",
    "#output: array([Actual NEG VAL PREDICTED how many WRONG, ACTUAL NEG VAL HOW MANY PREDICTED RIGHT,\n",
    "#                ACTUAL POS VAL PREDICTED how many WRONG, ACTUAL POS VAL HOW MANY PREDICTED RIGHT])\n",
    "            #Look at diagonal \n",
    "            # (negative) diagonal: \"ACTUAL TRUE predictions\"\n",
    "            # (positive) diagonal: \"PREDICTED that were wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Random Forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Rf_clf = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\", random_state=453)\n",
    "\n",
    "Rf_clf.fit(X_train,y_train)\n",
    "\n",
    "RandomForestClassifier(criterion=\"entropy\",random_state=101)\n",
    "preds = RF_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test,preds)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmls = dfml.copy()\n",
    "\n",
    "#Split data into X and y\n",
    "\n",
    "#DOUBLE CHECK capital X and small x, capital Y and small y, often a source of error\n",
    "X = dfmls.drop(\"HeartDisease\", axis = 1)\n",
    "y = dfmls[\"HeartDisease\"] # DO NORMALISATION AFTER SPLITTING!!!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Scaler = MinMaxScaler()\n",
    "Scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = Scaler.transform(X_train)\n",
    "x_test_scaled = Scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "LR_model = LogisticRegression(random_state = 42)\n",
    "\n",
    "LR_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "pres = LR_model.predict(X_test_scaled)\n",
    "\n",
    "acc = accuracy_score(y_test,preds)\n",
    "\n",
    "print(acc)\n",
    "\n",
    "\n",
    "#Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RF_clf = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\", random_state = 4)\n",
    "\n",
    "RF_clf.fit(X_train, y_train)\n",
    "\n",
    "preds = RF_clf.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
